{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Wind Power Dispatch Signals for AUS NEM Model\n",
    "Methods in [1] are adopted, whose source code can be found in [2]. Besides, data in 'df_n','df_g' are also downloaded from [2].<br/>\n",
    "This version was updated in Dec., 2020.\n",
    "\n",
    "#### This code aims to ####\n",
    "(1) Extract ***hourly*** load and wind/solar dispatch signals during 2015-2019 in Australia National Electricity Market (NEM) from AEMO's MMSDM database [3], to facilitate the case study in the AUS 912-node model;<br />\n",
    "(2) Fit the Gaussian Mixture Model(GMM) using the historical data (5 years, assumming that these data can be regarded as the population) (5years=43800 hours); <br/>\n",
    "(3) Sample small-scale datasets (20%,40%,60%,80% of **historical data** ) for constructing empirical distribution in the case studies; <br/>\n",
    "(4) Kmeans cluster of the small-scale datasets for reducing computational burden.<br/>\n",
    "\n",
    "##### NEM model #####\n",
    "In total, the NEM generator dataset contains technical and economic information relating to 203 generating units （conventional generators and renewable energy units, each has a dispatchable unit ID）, while the network dataset consists of 912 nodes, and 1406 AC edges with line voltages in the range of 110 kV to 500 kV. To \n",
    "\n",
    "At the same time, there are ***16*** NEM zones in the operation devided by AEMO. To facilitate our transmission network planning study, We take this 16-node model by assuming that the power balance constriants are met in each NEM zone.Therefore, the load and wind power signals are aggragated in these 16 nodes based on their geographic positions.\n",
    "\n",
    "##### IEEE RTS 24-node model #####\n",
    "In the original IEEE RTS 24-node model, it has 17 load nodes while no wind power. <br/>\n",
    "In the modified IEEE RTS 24-node model, technical and economic information relating to generators and topology of existing lines are not changed. However, to be compatible with the NEM dataset: <br/> (1) the load demand in IEEE RTS 24-node model at #13 node (reference node) is set to be zero (so that we get a 16-load model);  <br/> (2) #11,#12,#17,#24 are selected as nodes equipped with wind power.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from sklearn import mixture\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import scipy.io as sio\n",
    "from pyomo.environ import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import random\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import matrix_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Paths to directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data directory (common files)\n",
    "data_dir = os.path.abspath(os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, 'data'))\n",
    "\n",
    "# MMSDM data directory\n",
    "MMSDMdata_dir = os.path.join(data_dir,'AUS2016_2020')\n",
    "\n",
    "# Network directory\n",
    "network_dir = os.path.abspath(os.path.join(os.path.curdir, os.path.pardir, '1_network'))\n",
    "\n",
    "# Generators directory\n",
    "gens_dir = os.path.abspath(os.path.join(os.path.curdir, os.path.pardir, '2_generators'))\n",
    "\n",
    "# Output directory\n",
    "output_dir = os.path.abspath(os.path.join(os.path.curdir, 'output'))\n",
    "output_for_mat_dir=os.path.abspath(os.path.join(os.path.curdir, 'output_for_mat'))\n",
    "\n",
    "# basic DataFrame for network (index=NODE_ID)\n",
    "df_n = pd.read_csv(os.path.join(network_dir, 'output', 'network_nodes.csv'), index_col='NODE_ID', dtype={'NEAREST_NODE':np.int32})\n",
    "\n",
    "# basic DataFrame for generator (index=DUID)\n",
    "df_g = pd.read_csv(os.path.join(gens_dir, 'output', 'generators.csv'), dtype={'NODE': int}, index_col='DUID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define two functions for population (true distribution) and samples (empirical distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit GMM \n",
    "\n",
    "These GMM paramters would be used for measuring the \"distance\" between current probability distribution and future probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_forTrue_PDF(df_population):\n",
    "    # Fit the GMM and save the GMM parameters as distionary and mat. file\n",
    "    clf = mixture.GaussianMixture(n_components=3, covariance_type='full')\n",
    "    clf.fit(df_population)\n",
    "\n",
    "    GMM_True={}\n",
    "    GMM_True['Pr']=clf.weights_# Pr for each components\n",
    "    GMM_True['means']=clf.means_\n",
    "    GMM_True['Covariances']=clf.covariances_\n",
    "    return GMM_True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small-size sample and K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Kmeans(df_population):\n",
    "# Small-size samples of historical data for empirical distribution and K-means\n",
    "    SizeList=[0.2,0.4,0.6,0.8] # proportion of sample size to population\n",
    "    df_s={}\n",
    "    for sz in SizeList:        \n",
    "        sr=str(sz)  \n",
    "        sample=df_population.sample(frac=sz, replace=False, random_state=1)\n",
    "        df_s['Sample'+sr[2]]=sample.values\n",
    "        \n",
    "        #n_c=int(len(df_population)*sz*0.1)\n",
    "        n_c=1000\n",
    "        kmeans=MiniBatchKMeans(n_clusters=n_c).fit(sample)\n",
    "\n",
    "        df_s['Centers'+sr[2]]=kmeans.cluster_centers_\n",
    "        df_Pr=pd.value_counts(kmeans.labels_,sort=False,normalize=True)\n",
    "        df_s['Pr'+sr[2]]=df_Pr.values\n",
    "    return df_s\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small-size sample groups and data whitening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallgroup_white(df_population):\n",
    "# Small-size samples of historical data for empirical distribution and K-means\n",
    "    SizeList=[0.2,0.4,0.6,0.8] # proportion of sample size to population\n",
    "    df_diffsample={}\n",
    "    for sz in SizeList:    \n",
    "        # (1) Get a small size group (to show the influences of data size)\n",
    "        sr=str(sz)  \n",
    "        df_smallgroup=df_population.sample(frac=sz, replace=False, random_state=1)\n",
    "        df_diffsample['Sample'+sr[2]]=df_smallgroup.values\n",
    "        \n",
    "        #(2)  Statistics for empirical distribution\n",
    "        df_diffsample['Em_mean'+sr[2]]=df_smallgroup.mean().values\n",
    "        df_diffsample['Em_cov'+sr[2]]=df_smallgroup.cov().values\n",
    "        \n",
    "        #(3)  Standardization in each group\n",
    "        \n",
    "        U, S, V = np.linalg.svd(df_smallgroup.cov().values)\n",
    "        Xrot = np.dot(df_smallgroup.values, U)\n",
    "        df_diffsample['Xwhite'+sr[2]]=Xrot/np.sqrt(S+1e-5)        \n",
    "    return df_diffsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallgroup_normalize(df_population):\n",
    "# Small-size samples of historical data for empirical distribution and K-means\n",
    "    SizeList=[0.2,0.4,0.6,0.8,1.0] # proportion of sample size to population\n",
    "    df_diffsample={}\n",
    "    for sz in SizeList:    \n",
    "        # (1) Get a small size group (to show the influences of data size)\n",
    "        sr=str(sz)  \n",
    "        df_smallgroup=df_population.sample(frac=sz, replace=False, random_state=1)\n",
    "        df_diffsample['Sample'+sr[2]]=df_smallgroup.values\n",
    "        \n",
    "        #(2)  Statistics for empirical distribution\n",
    "        df_diffsample['Em_mean'+sr[2]]=df_smallgroup.mean().values\n",
    "        df_diffsample['Em_cov'+sr[2]]=df_smallgroup.cov().values\n",
    "        \n",
    "        #(3)  Standardization in each group\n",
    "        L=np.linalg.cholesky(df_Pv.cov().values)\n",
    "        invL=np.linalg.inv(L)\n",
    "        Zsample=np.dot(invL,(df_Pv-df_Pv.mean()).values.T)\n",
    "        df_diffsample['Zsample'+sr[2]]=Zsample        \n",
    "    return df_diffsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list=['0101','0201','0301','0401','0501','0601','0701','0801','0901','1001','1101','1201']\n",
    "year_list=['2016','2017','2018','2019','2020']\n",
    "Missing_list=['20160101','20170701','20170801','20170901','20200601','20200701','20201201']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dispatch data of wind power\n",
    "1. Every dispatched units have an ID (i.e., DUID) while we attached the fuel type from 'df_g', so that wind power dispatch data can be extracted;\n",
    "2. Parse and save unit dispatch data. Note that dispatch in MW is given at 5min intervals, and that the time resolution of demand data is 30min intervals, corresponding to the length of a trading period in the NEM. To align the time resolution of these signals unit dispatch data are aggregated, with mean power output over 60min intervals computed for each DUID.\n",
    "3. There are 912 nodes which are aggregated in 16 regions. Select ['TAS','NSA','CAN','MEL'] for the four wind farms in the modified IEEE RTS 24-node model;\n",
    "4. Fit the GMM and save the GMM parameters as distionary and mat file, for further optimization;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_zonal_wind_power (df_DISPATCH_UNIT_SCADA):\n",
    "    # Convert to datetime objects\n",
    "    df_DISPATCH_UNIT_SCADA['SETTLEMENTDATE'] = pd.to_datetime(df_DISPATCH_UNIT_SCADA['SETTLEMENTDATE'])\n",
    "    # Pivot dataframe. Dates are the index values, columns are DUIDs, values are DUID dispatch levels\n",
    "    df_DISPATCH_UNIT_SCADA_piv = df_DISPATCH_UNIT_SCADA.pivot(index='SETTLEMENTDATE', columns='DUID', values='SCADAVALUE')\n",
    "    # To ensure the 30th minute interval is included during each trading interval the time index is offset\n",
    "    # by 1min. Once the groupby operation is performed this offset is removed.\n",
    "    df_DISPATCH_UNIT_SCADA_agg = df_DISPATCH_UNIT_SCADA_piv.groupby(pd.Grouper(freq='60Min', base=1, label='right')).mean()\n",
    "    df_DISPATCH_UNIT_SCADA_agg = df_DISPATCH_UNIT_SCADA_agg.set_index(df_DISPATCH_UNIT_SCADA_agg.index - pd.Timedelta(minutes=1))\n",
    "    df_DISPATCH_UNIT_SCADA_agg\n",
    "    # (1)Nodal renewable energy system (RES) disaptch;(2)NEM Zonal RES dispatch\n",
    "    # Add fuel category to each DUID in SCADA dispatch dataframe\n",
    "    df_DISPATCH_UNIT_SCADA_agg = df_DISPATCH_UNIT_SCADA_agg.T.join(df_g[['FUEL_CAT']])\n",
    "    # Only consider intermittent solar and wind generators\n",
    "    mask = df_DISPATCH_UNIT_SCADA_agg['FUEL_CAT'].isin(['Wind'])\n",
    "    # Keep wind and solar (RES) DUIDs, drop fuel category column, and transpose (columns=DUID, index=Timestamp)\n",
    "    # All intermittent generation profiles\n",
    "    # (columns=DUID, index=Timestamps)\n",
    "    df_DUID_RES = df_DISPATCH_UNIT_SCADA_agg[mask].drop('FUEL_CAT', axis=1).T  \n",
    "\n",
    "    #(1) Nodal RES\n",
    "    # Add node to which generator is connected, groupby node and sum, \n",
    "    # reindex columns using all node IDs, yielding total intermittent injection at each node\n",
    "    # Injections from intermittent sources (columns=node ID, index=Timestamps)\n",
    "    df_nodal_RES=df_DUID_RES.T.join(df_g[['NODE']], how='left').groupby('NODE').sum().T.reindex(columns=df_n.index, fill_value=0)\n",
    "\n",
    "    return df_nodal_RES\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function for wind power data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160201\n",
      "20160301\n",
      "20160401\n",
      "20160501\n",
      "20160601\n",
      "20160701\n",
      "20160801\n",
      "20160901\n",
      "20161001\n",
      "20161101\n",
      "20161201\n",
      "20170101\n",
      "20170201\n",
      "20170301\n",
      "20170401\n",
      "20170501\n",
      "20170601\n",
      "20171001\n",
      "20171101\n",
      "20171201\n",
      "20180101\n",
      "20180201\n",
      "20180301\n",
      "20180401\n",
      "20180501\n",
      "20180601\n",
      "20180701\n",
      "20180801\n",
      "20180901\n",
      "20181001\n",
      "20181101\n",
      "20181201\n",
      "20190101\n",
      "20190201\n",
      "20190301\n",
      "20190401\n",
      "20190501\n",
      "20190601\n",
      "20190701\n",
      "20190801\n",
      "20190901\n",
      "20191001\n",
      "20191101\n",
      "20191201\n",
      "20200101\n",
      "20200201\n",
      "20200301\n",
      "20200401\n",
      "20200501\n",
      "20200801\n",
      "20200901\n",
      "20201001\n",
      "20201101\n"
     ]
    }
   ],
   "source": [
    "# Unit dispatch data\n",
    "#for file in os.listdir(dispatch_dir)\n",
    "\n",
    "\n",
    "df_wind=pd.DataFrame(columns = ['TAS','NSA','CAN','MEL']) \n",
    "\n",
    "for year in year_list:\n",
    "    for month in month_list:\n",
    "        a=year+''+month\n",
    "        if a in Missing_list:\n",
    "            continue\n",
    "        print(a)\n",
    "        name_csv='PUBLIC_DVD_DISPATCH_UNIT_SCADA_201901010000.CSV'\n",
    "        name_csv=name_csv.replace('0101',month)\n",
    "        name_csv=name_csv.replace('2019',year)\n",
    "        \n",
    "        df_DISPATCH_UNIT_SCADA = pd.read_csv(os.path.join(MMSDMdata_dir, name_csv),\n",
    "                                     skiprows=1, skipfooter=1, engine='python')\n",
    "    \n",
    "        df_wind0=get_zonal_wind_power (df_DISPATCH_UNIT_SCADA)\n",
    "        df_wind=df_wind.append(df_wind0)\n",
    "        \n",
    "#GMM_TrueWind=GMM_forTrue_PDF(df_wind)\n",
    "#Sample_Wind=sample_Kmeans(df_wind)\n",
    "#sio.savemat(os.path.join(output_for_mat_dir,'GMM_TrueWind.mat'), {'GMM_TrueWind': GMM_TrueWind})\n",
    "#sio.savemat(os.path.join(output_for_mat_dir,'Sample_Wind.mat'), {'Sample_Wind': Sample_Wind})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get load data\n",
    "1. Load data in each NEM region are given at 30min intervals. Likewise, demand data are aggregated, with mean values over 60min intervals computed for each region. <br />\n",
    "2. In the AEMO database, only trading information across the five regions, i.e., NSW, QLD,SA, VIC,TAS, are available. To obtain the load data for the 16 zones, we allocated the total load to each zone based on their respective population size, based on the information provided in [2] (i.e., in 'df_n')<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodal_load(df_TRADINGREGIONSUM_piv,df_n):\n",
    "    def node_demand(row, df_regd):\n",
    "        # row['NEM_REGION'] : define which trading region (the five) the node belongs to        \n",
    "            return (row['PROP_REG_D']+ 0.5*np.random.rand(1))* df_regd.loc[:, row['NEM_REGION']]\n",
    "        \n",
    "    df_nodal_d0=df_n.apply(node_demand, args=(df_TRADINGREGIONSUM_piv,), axis=1).T            \n",
    "    \n",
    "    return df_nodal_d0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function for load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: </br>\n",
    "As the detailed information for each NEM zone is unavaliable, we <br/> \n",
    "(1) allocate the load in each trading region to the zones based on population size in each zone (50%), i.e., df_load_pop <br/> \n",
    "(2) generate normal random variables based on practical mean and variance data (50%), i.e., df_loadrandom <br/> \n",
    "Otherwise, a non-sigular covariance matrix would be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160201\n",
      "20160301\n",
      "20160401\n",
      "20160501\n",
      "20160601\n",
      "20160701\n",
      "20160801\n",
      "20160901\n",
      "20161001\n",
      "20161101\n",
      "20161201\n",
      "20170101\n",
      "20170201\n",
      "20170301\n",
      "20170401\n",
      "20170501\n",
      "20170601\n",
      "20171001\n",
      "20171101\n",
      "20171201\n",
      "20180101\n",
      "20180201\n",
      "20180301\n",
      "20180401\n",
      "20180501\n",
      "20180601\n",
      "20180701\n",
      "20180801\n",
      "20180901\n",
      "20181001\n",
      "20181101\n",
      "20181201\n",
      "20190101\n",
      "20190201\n",
      "20190301\n",
      "20190401\n",
      "20190501\n",
      "20190601\n",
      "20190701\n",
      "20190801\n",
      "20190901\n",
      "20191001\n",
      "20191101\n",
      "20191201\n",
      "20200101\n",
      "20200201\n",
      "20200301\n",
      "20200401\n",
      "20200501\n",
      "20200801\n",
      "20200901\n",
      "20201001\n",
      "20201101\n"
     ]
    }
   ],
   "source": [
    "df_load_pop=pd.DataFrame()\n",
    "\n",
    "for year in year_list:\n",
    "    for month in month_list:\n",
    "        a=year+''+month\n",
    "        if a in Missing_list:\n",
    "            continue\n",
    "        print(a)\n",
    "        name_csv='PUBLIC_DVD_TRADINGREGIONSUM_201901010000.CSV'\n",
    "        name_csv=name_csv.replace('0101',month)\n",
    "        name_csv=name_csv.replace('2019',year)\n",
    "        # Regional summary for each trading interval\n",
    "        df_TRADINGREGIONSUM = pd.read_csv(os.path.join(MMSDMdata_dir, name_csv),\n",
    "                                      skiprows=1, skipfooter=1, engine='python')\n",
    "        # Convert settlement date to datetime\n",
    "        df_TRADINGREGIONSUM['SETTLEMENTDATE'] = pd.to_datetime(df_TRADINGREGIONSUM['SETTLEMENTDATE'])\n",
    "        # Pivot dataframe. Index is timestamp, columns are NEM region IDs, values are total demand\n",
    "        df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM.pivot(index='SETTLEMENTDATE', columns='REGIONID', values='TOTALDEMAND')\n",
    "        df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM_piv.groupby(pd.Grouper(freq='60Min', base=1, label='right')).mean()\n",
    "        df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM_piv.set_index(df_TRADINGREGIONSUM_piv.index - pd.Timedelta(minutes=1))\n",
    "\n",
    "        df_load0=get_nodal_load(df_TRADINGREGIONSUM_piv,df_n)\n",
    "        df_load_pop=df_load_pop.append(df_load0)\n",
    "        \n",
    "#df_load_pop=df_load_pop.T\n",
    "#np.linalg.matrix_rank(df_load_pop.cov().values)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=df_n['VOLTAGE_KV'].isin([275,330,400,500])\n",
    "df_load=df_load_pop.T[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load=df_load.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38856, 199)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu=df_load.mean().values\n",
    "sigma=np.diag(df_load.var().values)\n",
    "c,r=df_load.shape\n",
    "c,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_random=np.random.multivariate_normal(mu,sigma,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_final=df_load+load_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(df_load_final.cov().values)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM parameters for Pv=[Pload;Pwind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Pv = pd.concat([df_load_pop*0.7/300, df_wind], axis=1)\n",
    "# np.linalg.matrix_rank(df_Pv.cov().values)\n",
    "Pv_diffsample=samllgroup_normalize(df_Pv)\n",
    "sio.savemat(os.path.join(output_for_mat_dir,'Pv_diffsample2.mat'), {'Pv_diffsample': Pv_diffsample})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] -Xenophon, A., Hill, D. Open grid model of Australia’s National Electricity Market allowing backtesting against historic data. Sci Data 5, 180203 (2018). https://doi.org/10.1038/sdata.2018.203\n",
    "\n",
    "[2] -Xenophon, A. K. Geospatial Modelling of Australia’s National Electricity Market. GitHub https://github.com/akxen/egrimod-nem (2018).\n",
    "\n",
    "[3] -Australian Energy Markets Operator. Data Archive (2018). at http://www.nemweb.com.au/#mms-data-model \n",
    "\n",
    "[4] -Australian Energy Markets Operator. Data Archive (2018). at https://nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/\n",
    "\n",
    "#  Datasets List from MMSDM\n",
    "A summary of the tables used from AEMO's MMSDM database [3] is given below:\n",
    "\n",
    "| Table | Description |\n",
    "| :----- | :----- |\n",
    "|DISPATCH_UNIT_SCADA | MW dispatch at 5 minute (dispatch) intervals for DUIDs within the NEM.|\n",
    "|TRADINGREGIONSUM | Contains load in each NEM region at 30 minute (trading) intervals.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (egrimod-nem-env)",
   "language": "python",
   "name": "egrimod-nem-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
